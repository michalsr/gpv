exp_name: gpv_biatt_feats_vqa_only_multiple_answers_100_boxes_lr_mils_10_15_drop_0.5_answer_head_tied_bs_120_warmup_partial_credit_inf_mask_3072_interm_relevance
output_dir: /home/tanmayg/Data/gpv/coco_exp
exp_dir: ${output_dir}/${exp_name}
tb_dir: ${exp_dir}/tb_logs
ckpt_dir: ${exp_dir}/ckpts

gpu: 0
num_nodes: 1
ngpus_per_node: 4 #8
world_size: null  #computed dynamically as num_nodes*ngpus_per_node
rank: 0
workers: ${training.num_workers}
batch_size: ${training.batch_size}
dist_backend: nccl
dist_url: 'tcp://localhost:10001'
multiprocessing_distributed: True

hydra:
  run:
    dir: ${output_dir}/${exp_name}

defaults:
  - task: coco_learning_tasks

learning_datasets:
  # CocoCaptioning:
  #   task_config: coco_captioning
  #   name: coco_cap
  # CocoDetection:
  #   task_config: coco_detection
  #   name: coco_det
  # CocoClassification:
  #   task_config: coco_classification
  #   name: coco_cls
  CocoVqa:
    task_config: coco_vqa
    name: coco_vqa

model:
  pretr_detr: /home/tanmayg/Data/gpv/coco_exp/detr-r50-e632da11.pth
  vocab: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab.json
  vocab_embed: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab_embed.npy
  max_text_len: 5 #20
  answer_head: null
  detr:
    num_queries: 100
    num_classes: 1
    feat_dim: 2048
    hidden_dim: 768
    nheads: 8
    masks: False
    dilation: False
    dropout: 0.1
    aux_loss: True
    frozenbatchnorm: True
  bert_joiner:
    bert_dim: 768
    out_dim: ${model.detr.hidden_dim}
  vl_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  lv_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  text_decoder:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  co_att:
    visualization: False
    bi_num_attention_heads: 16
    bi_hidden_size: ${model.detr.hidden_dim}
    hidden_size: ${model.detr.hidden_dim}
    intermediate_size: 3072 #${model.detr.hidden_dim}
    output_size: ${model.detr.hidden_dim}
    attention_probs_dropout_prob: ${model.detr.dropout}
    hidden_dropout_prob: ${model.detr.dropout}
    hidden_act: gelu
    v_hidden_size: ${model.detr.hidden_dim}
    v_intermediate_size: 3072 #${model.detr.hidden_dim}
    v_output_size: ${model.detr.hidden_dim}
    v_attention_probs_dropout_prob: ${model.detr.dropout}
    v_hidden_dropout_prob: ${model.detr.dropout}
    v_hidden_act: gelu
    num_layers: 3
  losses: ${losses}

losses:
  AnswerClassification:
    name: answer_classification_criterion
    pad_idx: null
    loss_wts:
      loss_answer: 1

  Localization:
    name: localization_criterion
    cost_wts:
      ce: 1
      bbox: 5
      giou: 2
    loss_wts:
      loss_ce: 1
      loss_bbox: 5
      loss_giou: 2
    eos_coef: 0.1
    num_classes: ${model.detr.num_classes}

training:
  ckpt: null #${ckpt_dir}/model_bkp.pth #/home/tanmayg/Data/gpv/coco_exp/gpv/ckpts/frozen_model.pth
  freeze: False # freeze Detr layers
  frozen_epochs: 5
  frozen_batch_size: 64
  num_epochs: 40 # will be set to frozen_epochs if freeze is True
  batch_size: 120 #40 # will be set to frozen_batch_size if freeze is True
  num_workers: 30 #60 # 30
  vis_step: 2000
  log_step: 10
  ckpt_step: 2000
  lr: 1e-4
  weight_decay: 1e-4
  lr_milestones:
    - 10
    - 15
    - 20
    - 25
    - 30
    - 35
  lr_drop: 0.5
  lr_warmup: True
  clip_max_norm: 0 #0.1
  num_vis_samples: 15
  num_val_samples: 20000

eval:
  task: CocoVqa
  ckpt: ${exp_dir}/ckpts/model.pth
  batch_size: 20
  num_workers: 20
  subset: val
  predict: True
  num_eval_batches: null #1000 # set to null to evaluate on full dataset