exp_name: gpv_cap_det_cls_vqa_pretr_detr_last_hs_bs_${training.batch_size}_lr_${training.lr}_untie_dist
output_dir: /home/tanmayg/Data/gpv/coco_exp/
exp_dir: ${output_dir}/${exp_name}
tb_dir: ${exp_dir}/tb_logs
ckpt_dir: ${exp_dir}/ckpts

gpu: 0
ngpus_per_node: 2
world_size: 1
rank: 0
workers: ${training.num_workers}
batch_size: ${training.batch_size}
dist_backend: nccl
dist_url: 'tcp://localhost:10001'
distributed: null # to be set later
multiprocessing_distributed: True

hydra:
  run:
    dir: ${output_dir}/${exp_name}

defaults:
  - task: coco_learning_tasks

learning_datasets:
  CocoCaptioning:
    task_config: coco_captioning
    name: coco_cap
  CocoDetection:
    task_config: coco_detection
    name: coco_det
  CocoClassification:
    task_config: coco_classification
    name: coco_cls
  CocoVqa:
    task_config: coco_vqa
    name: coco_vqa

model:
  pretr_detr: /home/tanmayg/Data/gpv/coco_exp/detr-r50-e632da11.pth
  vocab: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab.json
  vocab_embed: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab_embed.npy
  max_text_len: 20
  detr:
    num_queries: 100
    num_classes: 1
    hidden_dim: 256
    nheads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    backbone: resnet50
    lr_backbone: 1e-4
    position_embedding: sine
    masks: False
    dilation: False
    dropout: 0.1
    dim_feedforward: 2048
    pre_norm: False #True
    aux_loss: True
    frozenbatchnorm: True
  bert_joiner:
    bert_dim: 768
    out_dim: ${model.detr.hidden_dim}
  vl_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  lv_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  text_decoder:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  losses: ${losses}


losses:
  AnswerClassification:
    name: answer_classification_criterion
    pad_idx: null
    loss_wts:
      loss_answer: 1

  Localization:
    name: localization_criterion
    cost_wts:
      ce: 1
      bbox: 5
      giou: 2
    loss_wts:
      loss_ce: 1
      loss_bbox: 5
      loss_giou: 2
    eos_coef: 0.1
    num_classes: ${model.detr.num_classes}

training:
  num_epochs: 30
  batch_size: 40 #10
  num_workers: 10 #20
  vis_step: 2000
  log_step: 10
  ckpt_step: 2000
  eval_epoch: 10
  lr: 1e-4
  weight_decay: 1e-4
  lr_drop: 15
  clip_max_norm: 0.1
  num_vis_samples: 15
  losses:
    - AnswerClassification
    - Localization