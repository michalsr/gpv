exp_name: gpv
output_dir: /home/tanmayg/Data/gpv/coco_exp/
exp_dir: ${output_dir}/${exp_name}
tb_dir: ${exp_dir}/tb_logs
ckpt_dir: ${exp_dir}/ckpts

gpu: 0
num_nodes: 1
ngpus_per_node: 8
world_size: null  #computed dynamically as num_nodes*ngpus_per_node
rank: 0
workers: ${training.num_workers}
batch_size: ${training.batch_size}
dist_backend: nccl
dist_url: 'tcp://localhost:10001'
multiprocessing_distributed: True

hydra:
  run:
    dir: ${output_dir}/${exp_name}

defaults:
  - task: coco_learning_tasks

learning_datasets:
  CocoCaptioning:
    task_config: coco_captioning
    name: coco_cap
  CocoDetection:
    task_config: coco_detection
    name: coco_det
  CocoClassification:
    task_config: coco_classification
    name: coco_cls
  CocoVqa:
    task_config: coco_vqa
    name: coco_vqa

model:
  pretr_detr: /home/tanmayg/Data/gpv/coco_exp/detr-r50-e632da11.pth
  vocab: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab.json
  vocab_embed: /home/tanmayg/Data/gpv/learning_phase_data/vocab/vocab_embed.npy
  max_text_len: 20
  detr:
    num_queries: 100
    num_classes: 1
    hidden_dim: 256
    nheads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    backbone: resnet50
    lr_backbone: 1e-5
    position_embedding: sine
    masks: False
    dilation: False
    dropout: 0.1
    dim_feedforward: 2048
    pre_norm: False #True
    aux_loss: True
    frozenbatchnorm: True
  bert_joiner:
    bert_dim: 768
    out_dim: ${model.detr.hidden_dim}
  vl_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  lv_transformer:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  text_decoder:
    hidden_dim: ${model.detr.hidden_dim}
    dropout: ${model.detr.dropout}
    nheads: ${model.detr.nheads}
    num_layers: 3
  losses: ${losses}

losses:
  AnswerClassification:
    name: answer_classification_criterion
    pad_idx: null
    loss_wts:
      loss_answer: 1

  Localization:
    name: localization_criterion
    cost_wts:
      ce: 1
      bbox: 5
      giou: 2
    loss_wts:
      loss_ce: 1
      loss_bbox: 5
      loss_giou: 2
    eos_coef: 0.1
    num_classes: ${model.detr.num_classes}

training:
  ckpt: ${ckpt_dir}/frozen_model.pth
  freeze: False # freeze Detr layers
  frozen_epochs: 5
  frozen_batch_size: 120
  num_epochs: 40 # will be set to frozen_epochs if freeze is True
  batch_size: 120 # will be set to frozen_batch_size if freeze is True
  num_workers: 60
  vis_step: 2000
  log_step: 10
  ckpt_step: 2000
  lr: 1e-4
  weight_decay: 1e-4
  lr_drop: 10
  clip_max_norm: 0.1
  num_vis_samples: 15