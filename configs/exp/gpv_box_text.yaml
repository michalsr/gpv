exp_name: gpv_box_cap_better_query_det_only_box_sup_vis_wo_teacher_forcing_relevance_fuse_hs_fuse_text_vocab_load #_factored_relevance_tokens_num_queries_${model.detr.num_queries}_lr_${training.lr}_lr_backbone_${model.detr.lr_backbone}_eos_${training.eos_coef}_clip_max_norm_${training.clip_max_norm}_max_samples_${task.clevr_query_detection_train.train.max_samples}_batch_size_${training.batch_size}
output_dir: /home/tanmayg/Data/gpv/exp
exp_dir: ${output_dir}/${exp_name}
tb_dir: ${exp_dir}/tb_logs
ckpt_dir: ${exp_dir}/ckpts

hydra:
  run:
    dir: ${output_dir}/${exp_name}

defaults:
  - task: clevr_learning_tasks

learning_datasets:
  ClevrQueryDetectionTrainTask:
    task_config: det
    name: clevr_det
  
  ClevrQuestionAnsweringTrainTask:
    task_config: qa
    name: clevr_qa

  ClevrCaptioningTrainTask:
    task_config: cap
    name: clevr_cap

model:
  vocab: /home/tanmayg/Data/gpv/clevr_min_objects_1_max_objects_3/train_task/vocab.json
  vocab_embed: /home/tanmayg/Data/gpv/clevr_min_objects_1_max_objects_3/train_task/vocab_embed.npy
  max_text_len: 6
  detr:
    num_queries: 50
    num_classes: 1
    hidden_dim: 256
    nheads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    backbone: resnet50
    lr_backbone: 1e-4
    position_embedding: sine
    masks: False
    dilation: False
    dropout: 0.1
    dim_feedforward: 2048
    pre_norm: True
    aux_loss: True
    frozenbatchnorm: False
    num_fusion_layers: 3

losses:
  AnswerClassification:
    name: answer_classification_criterion
    loss_wts:
      loss_answer: 1

  Localization:
    name: localization_criterion
    cost_wts:
      ce: 1
      bbox: 5
      giou: 2
    loss_wts:
      loss_ce: 1
      loss_bbox: 5
      loss_giou: 2
    eos_coef: 0.1
    num_classes: ${model.detr.num_classes}

training:
  num_epochs: 300 #1000 
  batch_size: 16
  num_workers: 8
  vis_step: 1000 #100
  log_step: 10
  ckpt_step: 1000
  eval_epoch: 10 #100
  lr: 1e-4
  weight_decay: 1e-4
  lr_drop: 150 #500 
  # cost_wts:
  #   ce: 1
  #   bbox: 5
  #   giou: 2
  # loss_wts:
  #   ce: 1
  #   bbox: 5
  #   giou: 2
  #   answer: 1
  # eos_coef: 0.1
  clip_max_norm: 0.1
  num_vis_samples: 24
  num_eval_samples: 
    train: 500
    val: null
    test: null
  losses:
    - AnswerClassification
    - Localization

# training:
#   num_epochs: 10000 #300
#   batch_size: 16
#   num_workers: 8
#   vis_step: 100 #500
#   log_step: 10 #100
#   ckpt_step: 1000 #500
#   eval_epoch: 200 #10
#   lr: 1e-4
#   weight_decay: 1e-4
#   lr_drop: 1000 #150
#   cost_wts:
#     ce: 1
#     bbox: 5
#     giou: 2
#   loss_wts:
#     ce: 1
#     bbox: 5
#     giou: 2
#   eos_coef: 1
#   clip_max_norm: 0.1
#   num_vis_samples: 16

eval:
  step: 18000
  subset: val
  num_eval_samples: null
  batch_size: ${training.batch_size}
  iou_thresh: 0.5
  