exp_name: cls_baseline
output_dir: /home/tanmayg/Data/gpv/coco_exp
exp_dir: ${output_dir}/${exp_name}
tb_dir: ${exp_dir}/tb_logs
ckpt_dir: ${exp_dir}/ckpts

gpu: 0
num_nodes: 1
ngpus_per_node: 4
world_size: null  #computed dynamically as num_nodes*ngpus_per_node
rank: 0
workers: ${training.num_workers}
batch_size: ${training.batch_size}
dist_backend: nccl
dist_url: 'tcp://localhost:10001'
multiprocessing_distributed: True

hydra:
  run:
    dir: ${output_dir}/${exp_name}

defaults:
  - task: coco_learning_tasks
  - learning_datasets: cls

training:
  ckpt: null # ${ckpt_dir}/model.pth
  freeze: False # freeze Detr layers
  frozen_epochs: 10
  frozen_batch_size: 120
  num_epochs: 40 # will be set to frozen_epochs if freeze is True
  batch_size: 120 # will be set to frozen_batch_size if freeze is True
  num_workers: 30
  vis_step: 2000
  log_step: 10
  ckpt_step: 2000
  lr: 1e-4
  lr_backbone: 1e-5
  weight_decay: 1e-4
  lr_milestones:
    - 10
    - 15
    - 20
    - 25
    - 30
    - 35
  lr_drop: 0.5
  lr_warmup: True
  lr_linear_decay: True
  lr_warmup_fraction: 0.1
  clip_max_norm: 0.1
  run_vis_at_launch: True
  num_vis_samples: 15
  run_eval_at_launch: True
  num_val_samples: 
    coco_cls: 10000